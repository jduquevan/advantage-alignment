resume: False
run_id: null
seed: 42
batch_size: 1024
wandb: 'online'
wandb_tags: ['meltingpot', 'PPO', 'aa']
wandb_dir: /network/scratch/j/juan.duque/wandb/
use_transformer: True
sum_rewards: False
self_play: True
share_encoder: True
use_reward_loss: False
use_spr_loss: True
device: cuda
rb_size: 84000
use_full_maps: False
hidden_size: 192
max_seq_len: 1000
max_cxt_len: 15
run_single_agent: False
debug_mode: False
use_ss_loss: True
agent_rb_size: 4000
agent_rb_mode: 'uniform-from-history'
on_policy_only: True
use_memmap: True
memmap_dir: /network/scratch/j/juan.duque/advantage-alignment/rb
store_wandb_video: False
sampled_agent_rb_size: 100

env:
  type: 'meltingpot'
  batch: 6
  device: ${device}
  scenario: 'commons_harvest__open'
  num_agents: 7
  max_env_steps: 1000

hydra:
  output_subdir: null
  run:
    dir: /network/scratch/j/juan.duque/hydra/

# Architecture hyperparameters
network:
  _target_: train.GruIPD
  device: ${device}

encoder:
  _target_: train.MeltingpotTransformer
  device: ${device}
  d_model: ${hidden_size}
  dim_feedforward: ${hidden_size}
  num_layers: 3
  max_seq_len: ${max_seq_len}
  attention_impl: 'original'
  init_weights: False

ss_module:
  _target_: train.SelfSupervisedModule
  n_actions: 8
  hidden_size: ${hidden_size}
  device: ${device}
  num_layers: 3
  mask_p: 0.15

mlp_model:
  _target_: train.LinearModel
  in_size: ${hidden_size}
  device: ${device}
  hidden_size: ${hidden_size}
  num_layers: 3
  out_size: 8
  init_weights: True

policy:
  _target_: train.CategoricalPolicy
  device: ${device}

linear_model:
  _target_: train.LinearModel
  in_size: ${hidden_size}
  hidden_size: ${hidden_size}
  out_size: 1
  device: ${device}
  num_layers: 3
  init_weights: False

optimizer_actor:
  _target_: torch.optim.Adam
  lr: 0.00001

optimizer_critic:
  _target_: torch.optim.Adam
  lr: 0.00001

optimizer_ss:
  _target_: torch.optim.Adam
  lr: 0.00001

agent:
  _target_: train.AdvantageAlignmentAgent
  device: ${device}

training:
  gamma: 0.99
  tau: 0.005
  max_traj_len: ${max_seq_len}
  batch_size: 128 # Training batch size
  total_steps: 10000000 # Number of optimization steps to train for
  eval_every: 100 # Number of optimization steps before evaluation
  proximal: True
  clip_range: 0.1
  entropy_beta: 0.1
  updates_per_batch: 2
  kl_threshold: 0.05
  use_mlm: True
  critic_loss_mode: 'td-1'
  save_every: 100
  video_dir: '/home/mila/j/juan.duque/scratch/advantage-alignment/videos'
  checkpoint_dir: '/home/mila/j/juan.duque/scratch/advantage-alignment/experiments'
  aa_weight: 0
  clip_grad_norm: 10
  ss_epochs: 1
  id_weight: 0
  spr_weight: 1
  normalize_advantages: True
  actor_loss_mode: 'integrated_aa'
  add_to_replay_buffer_every: 20
  sample_from_replay_buffer_every: 10
  add_to_agent_replay_buffer_every: 20
  num_on_policy_agents: 5
  center_rewards: False
  augment: False

