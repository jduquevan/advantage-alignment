{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from decoder import TransformerConfig, RLTransformer\n",
    "import torch"
   ],
   "id": "f1c8ed1e02a86d54",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "config = TransformerConfig(max_seq_len=100,\n",
    "                  attention='causal',\n",
    "                  attention_impl='scaled_dot',  # original\n",
    "                  num_layers=3,\n",
    "                  num_heads=4,\n",
    "                  embed_dim=128,\n",
    "                  embed_pdrop=0.0, \n",
    "                  resid_pdrop=0.0, \n",
    "                  attn_pdrop=0.0,)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transformer = RLTransformer(config)\n",
    "transformer"
   ],
   "id": "2644e2806f7c623e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output, new_kv = transformer(torch.rand(3, 73, 128))",
   "id": "933534e686c3248a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----- test no randomness in the forward -----\n",
    "transformer = RLTransformer(config)\n",
    "x = torch.rand(3, 73, 128)\n",
    "output1, kv1 = transformer(x)\n",
    "output2, kv2 = transformer(x)\n",
    "assert torch.allclose(output1, output2)\n",
    "assert torch.allclose(kv1['k'], kv2['k'])\n",
    "assert torch.allclose(kv1['v'], kv2['v'])"
   ],
   "id": "5ffd592a8e6cf7a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output1",
   "id": "bb62937099013fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----- test the kv cache vs no kv cache, should be the same -----\n",
    "# no kv cache\n",
    "transformer = RLTransformer(config)\n",
    "B = 17 # batch size\n",
    "T = 33 # sequence length\n",
    "D = 128 # embedding dimension\n",
    "x = torch.rand(B, T, D)\n",
    "output1, kv1 = transformer(x) # output1: (B, T, D)\n",
    "\n",
    "# with kv cache\n",
    "kv_cache = transformer.generate_empty_keys_values(n=B, max_tokens=T)\n",
    "# feed the x one by one\n",
    "output2s = []\n",
    "for t in range(x.shape[1]):\n",
    "    past_ks = torch.stack([layer_cache.get()[0] for layer_cache in kv_cache._keys_values], dim=0)\n",
    "    past_vs = torch.stack([layer_cache.get()[1] for layer_cache in kv_cache._keys_values], dim=0)\n",
    "    output2, this_kvs = transformer(x[:, t:t+1], past_keys_values=(past_ks, past_vs)) # output2: (B, 1, D)\n",
    "    assert this_kvs['k'].shape == (len(transformer.blocks), B, config.num_heads, 1, config.embed_dim // config.num_heads)\n",
    "    assert this_kvs['v'].shape == (len(transformer.blocks), B, config.num_heads, 1, config.embed_dim // config.num_heads)\n",
    "\n",
    "    # update the kv_caches\n",
    "    for j, layer_cache in enumerate(kv_cache._keys_values):  # looping over caches for layers\n",
    "        assert layer_cache._k_cache.shape == (B, config.num_heads, t, config.embed_dim // config.num_heads)\n",
    "        assert layer_cache._v_cache.shape == (B, config.num_heads, t, config.embed_dim // config.num_heads)\n",
    "        this_layer_k = this_kvs['k'][j]\n",
    "        this_layer_v = this_kvs['v'][j]\n",
    "        layer_cache.update(this_layer_k, this_layer_v)\n",
    "    \n",
    "    output2s.append(output2)\n",
    "\n",
    "output2 = torch.cat(output2s, dim=1)\n",
    "# check the equivalent of the outputs\n",
    "print(f\"mean error: {torch.mean(torch.abs(output1 - output2))}, shapes: {output1.shape}, {output2.shape}\")\n",
    "assert torch.allclose(output1, output2, atol=1e-6)\n",
    "# check the equivalent of the kv\n",
    "past_ks = torch.stack([layer_cache.get()[0] for layer_cache in kv_cache._keys_values], dim=0)\n",
    "past_vs = torch.stack([layer_cache.get()[1] for layer_cache in kv_cache._keys_values], dim=0)\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv1['k'] - past_ks))}, shapes: {kv1['k'].shape}, {past_ks.shape}\")\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv1['v'] - past_vs))}, shapes: {kv1['v'].shape}, {past_vs.shape}\")\n",
    "assert torch.allclose(kv1['k'], past_ks, atol=1e-6)\n",
    "assert torch.allclose(kv1['v'], past_vs, atol=1e-6)"
   ],
   "id": "16b88cf3140069f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----- test the vmapped function forward with kv cache and no kv cache -----\n",
    "import copy\n",
    "# test the vmapped forward method\n",
    "from torch.func import functional_call, stack_module_state\n",
    "def call_models_forward(models, xs, past_ks_vs=None):\n",
    "    # Stack all agent parameters\n",
    "    params, buffers = stack_module_state(models)\n",
    "    base_model = copy.deepcopy(models[0])\n",
    "    base_model = base_model.to('meta')\n",
    "\n",
    "    def fmodel(params, buffers, x, past_ks_vs=None):\n",
    "        batched = False\n",
    "        return functional_call(base_model, (params, buffers), (x, past_ks_vs, batched))\n",
    "\n",
    "    if past_ks_vs is not None:\n",
    "        vmap_fmodel = torch.vmap(fmodel, in_dims=(0, 0, 0, 0), randomness='different')(\n",
    "            params, buffers, xs, past_ks_vs)\n",
    "    else:\n",
    "        vmap_fmodel = torch.vmap(fmodel, in_dims=(0, 0, 0), randomness='different')(\n",
    "            params, buffers, xs)\n",
    "\n",
    "    return vmap_fmodel\n",
    "# no kv cache and no vmapped forward\n",
    "print(\"-\"*50)\n",
    "print(\"no kv cache and no vmapped forward\")\n",
    "transformer = RLTransformer(config)\n",
    "B = 17 # batch size\n",
    "T = 33 # sequence length\n",
    "D = 128 # embedding dimension\n",
    "xs = torch.rand(B, T, D)\n",
    "output1, kv1 = transformer(xs)\n",
    "\n",
    "# no kv cache but with vmapped forward\n",
    "print(\"-\"*50)\n",
    "print(\"no kv cache but with vmapped forward\")\n",
    "formers = [transformer for _ in range(B)]\n",
    "output2, kv2 = call_models_forward(formers, xs.unsqueeze(1)) # unsqueeze to add a fake batch dimension\n",
    "output2 = output2.squeeze(1) # remove the fake batch dimension, later refactor so that the fake batch dimension is removed already\n",
    "print(f\"shapes of kvs: {kv1['k'].shape}, {kv2['k'].shape}\")\n",
    "kv2['k'] = kv2['k'].transpose(0, 1) # (B, num_layers, num_heads, T, D // num_heads) -> (num_layers, B, num_heads, T, D // num_heads)\n",
    "kv2['v'] = kv2['v'].transpose(0, 1) # (B, num_layers, num_heads, T, D // num_heads) -> (num_layers, B, num_heads, T, D // num_heads)\n",
    "print(f\"mean error: {torch.mean(torch.abs(output1 - output2))}, shapes: {output1.shape}, {output2.shape}\")\n",
    "assert torch.allclose(output1, output2, atol=1e-6)\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv1['k'] - kv2['k']))}, shapes: {kv1['k'].shape}, {kv2['k'].shape}\")\n",
    "assert torch.allclose(kv1['k'], kv2['k'], atol=1e-6)\n",
    "\n",
    "# with kv cache and vmapped forward \n",
    "print(\"-\"*50)\n",
    "print(\"with kv cache and vmapped forward\")\n",
    "kv_cache = formers[0].generate_empty_keys_values(n=B, max_tokens=T)\n",
    "output3_s = []\n",
    "for t in range(xs.shape[1]):\n",
    "    past_ks = torch.stack([layer_cache.get()[0] for layer_cache in kv_cache._keys_values], dim=1) # stack in dimension 1 so vmap works on dimension 0 (batch dimension)\n",
    "    past_vs = torch.stack([layer_cache.get()[1] for layer_cache in kv_cache._keys_values], dim=1)\n",
    "    assert past_ks.shape == (B, len(formers[0].blocks), config.num_heads, t, config.embed_dim // config.num_heads)\n",
    "    assert past_vs.shape == (B, len(formers[0].blocks), config.num_heads, t, config.embed_dim // config.num_heads)\n",
    "    \n",
    "    output3, this_kvs = call_models_forward(formers, xs[:, t:t+1].unsqueeze(1), (past_ks, past_vs))\n",
    "    output3 = output3.squeeze(1) # remove the fake batch dimension so (B, 1, 1, D) -> (B, 1, D); 1 is the sequence length in the second shape\n",
    "    output3_s.append(output3) \n",
    "    assert this_kvs['k'].shape == (B, len(formers[0].blocks), config.num_heads, 1, config.embed_dim // config.num_heads)\n",
    "    assert this_kvs['v'].shape == (B, len(formers[0].blocks), config.num_heads, 1, config.embed_dim // config.num_heads)\n",
    "    \n",
    "    # update the kv_caches\n",
    "    for j, layer_cache in enumerate(kv_cache._keys_values):  # looping over caches for layers\n",
    "        this_layer_k = this_kvs['k'][:, j]\n",
    "        this_layer_v = this_kvs['v'][:, j]\n",
    "        layer_cache.update(this_layer_k, this_layer_v)\n",
    "    \n",
    "output3 = torch.cat(output3_s, dim=1)\n",
    "# check the equivalent of the outputs\n",
    "print(f\"mean error: {torch.mean(torch.abs(output1 - output3))}, shapes: {output1.shape}, {output3.shape}\")\n",
    "assert torch.allclose(output1, output3, atol=1e-6)\n",
    "# check the equivalent of the kv\n",
    "past_ks = torch.stack([layer_cache.get()[0] for layer_cache in kv_cache._keys_values], dim=1)\n",
    "past_vs = torch.stack([layer_cache.get()[1] for layer_cache in kv_cache._keys_values], dim=1)\n",
    "past_ks = past_ks.transpose(0, 1) # (B, num_layers, num_heads, T, D // num_heads) -> (num_layers, B, num_heads, T, D // num_heads)\n",
    "past_vs = past_vs.transpose(0, 1) # (B, num_layers, num_heads, T, D // num_heads) -> (num_layers, B, num_heads, T, D // num_heads)\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv1['k'] - past_ks))}, shapes: {kv1['k'].shape}, {past_ks.shape}\")\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv1['v'] - past_vs))}, shapes: {kv1['v'].shape}, {past_vs.shape}\")\n",
    "assert torch.allclose(kv1['k'], past_ks, atol=1e-6)\n",
    "assert torch.allclose(kv1['v'], past_vs, atol=1e-6)"
   ],
   "id": "b4d3402315cbae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test the scaled_dot_product_attention function and compare it with the manual implementation\n",
    "\n",
    "config_original = TransformerConfig(max_seq_len=100,\n",
    "                    attention='causal',\n",
    "                    attention_impl='original',\n",
    "                    num_layers=3,\n",
    "                    num_heads=4,\n",
    "                    embed_dim=128,\n",
    "                    embed_pdrop=0.0, \n",
    "                    resid_pdrop=0.0, \n",
    "                    attn_pdrop=0.0,)\n",
    "\n",
    "transformer_original = RLTransformer(config_original)\n",
    "\n",
    "config_scaled_dot = TransformerConfig(max_seq_len=100,\n",
    "                  attention='causal',\n",
    "                  attention_impl='scaled_dot',\n",
    "                  num_layers=3,\n",
    "                  num_heads=4,\n",
    "                  embed_dim=128,\n",
    "                  embed_pdrop=0.0, \n",
    "                  resid_pdrop=0.0, \n",
    "                  attn_pdrop=0.0,)\n",
    "\n",
    "transformer_scaled_dot = RLTransformer(config_scaled_dot)\n",
    "# use state_dict of original\n",
    "transformer_scaled_dot.load_state_dict(transformer_original.state_dict())\n",
    "\n",
    "# original \n",
    "x = torch.rand(3, 73, 128)\n",
    "output_original, kv_original = transformer_original(x)\n",
    "\n",
    "# scaled dot\n",
    "output_scaled_dot, kv_scaled_dot = transformer_scaled_dot(x)\n",
    "\n",
    "print(f\"mean error: {torch.mean(torch.abs(output_original - output_scaled_dot))}, shapes: {output_original.shape}, {output_scaled_dot.shape}\")\n",
    "assert torch.allclose(output_original, output_scaled_dot, atol=1e-6)\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv_original['k'] - kv_scaled_dot['k']))}, shapes: {kv_original['k'].shape}, {kv_scaled_dot['k'].shape}\")\n",
    "assert torch.allclose(kv_original['k'], kv_scaled_dot['k'], atol=1e-6)\n",
    "print(f\"mean error: {torch.mean(torch.abs(kv_original['v'] - kv_scaled_dot['v']))}, shapes: {kv_original['v'].shape}, {kv_scaled_dot['v'].shape}\")\n",
    "assert torch.allclose(kv_original['v'], kv_scaled_dot['v'], atol=1e-6)"
   ],
   "id": "650911610b36bc75",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
